{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7e1251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch can use your GPU.\n",
      "Number of GPUs available: 1\n",
      "GPU Name: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can use your GPU.\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\") # Prints the name of the first GPU\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a895d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "\n",
    "    def __init__(self, json_file):\n",
    "\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data_dict = json.load(f)\n",
    "        \n",
    "        self.data = self.data_dict['data']\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        goal = np.array([\n",
    "            item['config']['goal']['x'],\n",
    "            item['config']['goal']['y'],\n",
    "            item['config']['goal']['z']\n",
    "        ])\n",
    "        \n",
    "        config = goal # Label, Shape (3, )\n",
    "        \n",
    "        waypoints = []\n",
    "        for wp in item['trajectory']['waypoints']:\n",
    "            waypoints.append([\n",
    "                wp['x'], wp['y'], wp['z'],\n",
    "                wp['qx'], wp['qy'], wp['qz'], wp['qw']\n",
    "            ])\n",
    "        trajectory = np.array(waypoints)  # Output Data, Shape: (seq_len, 7)\n",
    "        \n",
    "        return torch.FloatTensor(config), torch.FloatTensor(trajectory)\n",
    "    \n",
    "\n",
    "def collate_pad(batch):\n",
    "    xs, ys = zip(*batch)                 # tuples\n",
    "    xs = torch.stack(xs, dim=0)          # [B, 3]\n",
    "\n",
    "    \n",
    "    lengths = torch.tensor([y.size(0) for y in ys], dtype=torch.long)\n",
    "    ys = pad_sequence(ys, batch_first=True) \n",
    "\n",
    "    B, T_max, _ = ys.shape\n",
    "\n",
    "    mask = torch.zeros(B, T_max, dtype=torch.bool)\n",
    "\n",
    "    for b, L in enumerate(lengths):\n",
    "        mask[b, :L] = True\n",
    "\n",
    "    return xs, ys, lengths, mask  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_trajectory_generator(nn.Module):\n",
    "    ''' Decodes hidden state output by encoder '''\n",
    "    \n",
    "    def __init__(self, input_size, trans_size, quat_size, hidden_size, num_layers):\n",
    "\n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n",
    "        :                       2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(lstm_trajectory_generator, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers)\n",
    "        self.translation_linear = nn.Linear(hidden_size, trans_size)\n",
    "\n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "        \n",
    "        '''        \n",
    "        : param x_input:                    should be 2D (batch_size, input_size)\n",
    "        : param encoder_hidden_states:      hidden states\n",
    "        : return output, hidden:            output gives all the hidden states in the sequence;\n",
    "        :                                   hidden gives the hidden state and cell state for the last\n",
    "        :                                   element in the sequence \n",
    " \n",
    "        '''\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x_input.unsqueeze(0), encoder_hidden_states) \n",
    "        translation_pred = self.translation_linear(lstm_out.squeeze(0))\n",
    "                \n",
    "        return translation_pred, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(\n",
    "    model,\n",
    "    goal,\n",
    "    consecutive,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "):\n",
    "    \"\"\"\n",
    "    Roll out trajectories for a batch of goals using convergence + proximity\n",
    "    stopping rules and safety caps.\n",
    "\n",
    "    Args:\n",
    "        model: LSTM trajectory generator. Called as model(goal, hidden).\n",
    "        goal:  (B, 3) tensor of goal positions.\n",
    "        consecutive: int, number of consecutive small steps required\n",
    "                     to declare convergence.\n",
    "        device: torch device.\n",
    "\n",
    "    Returns:\n",
    "        pred_traj:  (B, T_used, 2) predicted x,y waypoints (padded if needed).\n",
    "        lengths:   (B,) tensor of actual lengths for each trajectory\n",
    "                   (number of valid steps per batch element).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    goal = goal.to(device)\n",
    "    B = goal.size(0)\n",
    "\n",
    "    # Hyperparameters for stopping\n",
    "    eps_step = 0.01   # [m] step size below which we consider it \"not moving\" (~1 cm)\n",
    "    eps_goal = 0.02   # [m] distance to goal considered \"close enough\" (~5 cm)\n",
    "    T_max = 100     # max number of rollout steps (tune to your dataset)\n",
    "    L_max = 2.0     # [m] max total path length (your earlier constraint)\n",
    "\n",
    "    goal_xy = goal[:, :2]  # (B, 2), we only predict x,y for now\n",
    "\n",
    "    # Storage\n",
    "    pred_traj = torch.zeros(B, T_max, 2, device=device)\n",
    "\n",
    "    # Per-sample bookkeeping\n",
    "    lengths = torch.zeros(B, dtype=torch.long, device=device)   # final lengths\n",
    "    finished = torch.zeros(B, dtype=torch.bool, device=device)   # done mask\n",
    "    stable_counts = torch.zeros(B, dtype=torch.long, device=device)   # consecutive small-step count\n",
    "    total_length = torch.zeros(B, device=device)                     # path length so far\n",
    "\n",
    "    hidden = None\n",
    "    prev_pos = None\n",
    "    last_step = 0  # will track how many timesteps we actually used globally\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in range(T_max):\n",
    "\n",
    "            # LSTM step: input is just the goal (same every step)\n",
    "            trans_pred, hidden = model(goal, hidden)   # (B, 2)\n",
    "            pred_traj[:, t, :] = trans_pred\n",
    "            last_step = t + 1\n",
    "\n",
    "            # Step size & path length\n",
    "            if t > 0:\n",
    "                step_vec  = trans_pred - prev_pos           # (B, 2)\n",
    "                step_norm = torch.norm(step_vec, dim=-1)    # (B,)\n",
    "\n",
    "                # If ANY component of the step is negative, treat it as \"backwards\"\n",
    "                # and subtract the norm from total_length instead of adding it.\n",
    "                moving_backward = (step_vec < 0).any(dim=-1)        # (B,) bool\n",
    "                signed_step = torch.where(moving_backward,\n",
    "                                        -step_norm,               # subtract\n",
    "                                        step_norm)                # add\n",
    "                total_length += signed_step\n",
    "\n",
    "                # Update \"stable\" counts (small movement)\n",
    "                is_small_step = step_norm < eps_step\n",
    "                stable_counts = torch.where(\n",
    "                    is_small_step,\n",
    "                    stable_counts + 1,\n",
    "                    torch.zeros_like(stable_counts),\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                # First step: no previous position yet\n",
    "                step_norm = torch.zeros(B, device=device)\n",
    "                \n",
    "            # Distance to goal\n",
    "            dist_to_goal = torch.norm(trans_pred - goal_xy, dim=-1)  # (B,)\n",
    "\n",
    "            # Termination criteria\n",
    "            close_enough   = dist_to_goal < eps_goal\n",
    "            stable_enough  = stable_counts >= consecutive\n",
    "            converged_good = close_enough & stable_enough\n",
    "\n",
    "            too_long_path  = total_length > L_max\n",
    "\n",
    "            # For each element not already finished, see if it should now stop\n",
    "            newly_done = (~finished) & (converged_good | too_long_path)\n",
    "\n",
    "            # Set length for newly finished sequences (t is 0-indexed, so length = t+1)\n",
    "            lengths = torch.where(\n",
    "                newly_done & (lengths == 0),\n",
    "                torch.full_like(lengths, t + 1),\n",
    "                lengths,\n",
    "            )\n",
    "\n",
    "            finished = finished | newly_done\n",
    "            prev_pos = trans_pred\n",
    "\n",
    "            # If all samples are done, we can stop unrolling early\n",
    "            if finished.all():\n",
    "                break\n",
    "\n",
    "        # For any samples that never met a stopping condition, we\n",
    "        # treat their length as the number of steps we actually unrolled.\n",
    "        lengths = torch.where(\n",
    "            lengths == 0,\n",
    "            torch.full_like(lengths, last_step),\n",
    "            lengths,\n",
    "        )\n",
    "\n",
    "    # Trim padded trajectory tensor to the actually used global horizon\n",
    "    pred_traj = pred_traj[:, :last_step, :]  # (B, last_step, 2)\n",
    "\n",
    "    return pred_traj, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ab171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, \n",
    "          epochs, learning_rate,\n",
    "          device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    params = model.parameters()\n",
    "    optimizer = optim.Adam(params, lr=learning_rate)\n",
    "    state_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "    }\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        training_loss = 0.0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "\n",
    "            goal = batch[0].to(device)\n",
    "            gt_traj = batch[1].to(device) # (batch_size, max_batch_traj_length, 7)\n",
    "            gt_lengths = batch[2].to(device)\n",
    "            gt_mask = batch[3].to(device)\n",
    "            batch_size, max_traj_len, _= gt_traj.shape\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            gt_translation = gt_traj[:, :, :2]  # (B, T, 2) - just x, y\n",
    "\n",
    "            hidden = None\n",
    "\n",
    "            pred_translations = torch.zeros(batch_size, max_traj_len, 2).to(device) # (B, T, 2)\n",
    "\n",
    "            for t in range(max_traj_len):\n",
    "                trans_pred, hidden = model(goal, hidden)\n",
    "                pred_translations[:, t, :] = trans_pred\n",
    "\n",
    "            \n",
    "            trans_loss = state_loss(pred_translations, gt_translation)  # (B, T, 2)\n",
    "            trans_loss = trans_loss.mean(-1)  # Average over x, y -> (B, T)\n",
    "            #trans_loss_masked = (trans_loss * gt_mask).sum() / gt_mask.sum()\n",
    "\n",
    "            _, T_max, _ = pred_translations.shape\n",
    "            time_weights = torch.linspace(1.0, 5.0, steps=T_max, device=device)  # for example\n",
    "            time_weights = time_weights.unsqueeze(0)  # (1, T_max)\n",
    "            weighted_loss = trans_loss * time_weights * gt_mask\n",
    "\n",
    "\n",
    "            # if epoch % 25 == 0 and i == 0:  # First batch every 20 epochs\n",
    "                \n",
    "            #     # Translation predictions\n",
    "            #     print(\"\\n=== Translation Predictions vs Ground Truth ===\")\n",
    "                \n",
    "            #     # Track all errors across the batch\n",
    "            #     all_batch_errors = []\n",
    "            #     trajectory_means = []\n",
    "                \n",
    "            #     for b in range(batch_size):  # Show first 3 trajectories\n",
    "            #         valid_length = int(gt_lengths[b].item())\n",
    "                    \n",
    "            #         if valid_length > 0:\n",
    "            #             print(f\"\\nTrajectory {b} (length={valid_length}):\")\n",
    "            #             print(\"Step |  Pred (x, y)    |   GT (x, y)     |  Error\")\n",
    "            #             print(\"-\" * 55)\n",
    "                        \n",
    "            #             # Track errors for mean calculation\n",
    "            #             trajectory_errors = []\n",
    "                        \n",
    "            #             # Show entire trajectory\n",
    "            #             for t in range(valid_length):\n",
    "            #                 pred_x = pred_translations[b, t, 0].item()\n",
    "            #                 pred_y = pred_translations[b, t, 1].item()\n",
    "            #                 gt_x = gt_translation[b, t, 0].item()\n",
    "            #                 gt_y = gt_translation[b, t, 1].item()\n",
    "                            \n",
    "            #                 error = (pred_x - gt_x)**2 + (pred_y - gt_y)**2\n",
    "            #                 trajectory_errors.append(error)\n",
    "            #                 all_batch_errors.append(error)\n",
    "                            \n",
    "            #                 print(f\"{t:4d} | ({pred_x:6.3f}, {pred_y:6.3f}) | \"\n",
    "            #                     f\"({gt_x:6.3f}, {gt_y:6.3f}) | {error:6.4f}\")\n",
    "                        \n",
    "            #             # Print mean error for this trajectory\n",
    "            #             mean_error = sum(trajectory_errors) / len(trajectory_errors)\n",
    "            #             trajectory_means.append(mean_error)\n",
    "            #             print(\"-\" * 55)\n",
    "            #             print(f\"Mean trajectory error: {mean_error:6.4f}\")\n",
    "                \n",
    "            #     # Print both types of batch mean errors\n",
    "            #     print(\"=\" * 55)\n",
    "            #     if all_batch_errors:\n",
    "            #         batch_mean_error = sum(all_batch_errors) / len(all_batch_errors)\n",
    "            #         print(f\"BATCH MEAN ERROR (all timesteps): {batch_mean_error:6.4f}\")\n",
    "\n",
    "            #     if trajectory_means:\n",
    "            #         mean_of_means = sum(trajectory_means) / len(trajectory_means)\n",
    "            #         print(f\"BATCH MEAN ERROR (mean of means): {mean_of_means:6.4f}\")\n",
    "\n",
    "\n",
    "            loss = weighted_loss.sum() / (time_weights * gt_mask).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item()\n",
    "\n",
    "        average_train_loss = training_loss/len(train_loader)\n",
    "        history['train_loss'].append(average_train_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}: Train Loss = {average_train_loss:.4f}')\n",
    "\n",
    "        # Validation #\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_loss = 0.0\n",
    "\n",
    "            for i, batch in enumerate(val_loader):\n",
    "\n",
    "                goal      = batch[0].to(device)   # (B, 3)\n",
    "                gt_traj   = batch[1].to(device)   # (B, T_gt_max, 7)\n",
    "                gt_lengths = batch[2].to(device)  # (B,)\n",
    "\n",
    "                batch_size, max_traj_len, _ = gt_traj.shape\n",
    "\n",
    "                pred_traj, pred_len = rollout(model, goal, consecutive=3, device=device)\n",
    "                # pred_traj: (B, T_pred_max, 2)\n",
    "                # pred_len:  (B,)\n",
    "\n",
    "                # We'll just look at x,y from the GT trajectory\n",
    "                gt_translation = gt_traj[:, :, :2]\n",
    "\n",
    "                # === DEBUG PRINTS ===\n",
    "                # First batch every 25 epochs (mirroring your training logic)\n",
    "                if epoch % 25 == 0 and i == 0:\n",
    "\n",
    "                    print(\"\\n=== [VALIDATION] Translation Predictions vs Ground Truth ===\")\n",
    "\n",
    "                    # Show up to first 3 trajectories in this batch\n",
    "                    for b in range(min(batch_size, 3)):\n",
    "                        gt_L    = int(gt_lengths[b].item())\n",
    "                        pred_L  = int(pred_len[b].item())\n",
    "                        max_L   = max(gt_L, pred_L)\n",
    "\n",
    "                        print(f\"\\nTrajectory {b} (GT length={gt_L}, Pred length={pred_L}):\")\n",
    "                        print(\"Step |    Pred (x, y)      |     GT (x, y)       |  Error\")\n",
    "                        print(\"-\" * 70)\n",
    "\n",
    "                        for t in range(max_L):\n",
    "                            # Predicted point (if within predicted length)\n",
    "                            if t < pred_L:\n",
    "                                pred_x = pred_traj[b, t, 0].item()\n",
    "                                pred_y = pred_traj[b, t, 1].item()\n",
    "                                pred_str = f\"({pred_x:7.3f}, {pred_y:7.3f})\"\n",
    "                            else:\n",
    "                                pred_x = pred_y = None\n",
    "                                pred_str = \"      ---           \"\n",
    "\n",
    "                            # Ground-truth point (if within GT length)\n",
    "                            if t < gt_L:\n",
    "                                gt_x = gt_translation[b, t, 0].item()\n",
    "                                gt_y = gt_translation[b, t, 1].item()\n",
    "                                gt_str = f\"({gt_x:7.3f}, {gt_y:7.3f})\"\n",
    "                            else:\n",
    "                                gt_x = gt_y = None\n",
    "                                gt_str = \"      ---           \"\n",
    "\n",
    "                            # Per-step squared error only if both exist\n",
    "                            if (pred_x is not None) and (gt_x is not None):\n",
    "                                err = (pred_x - gt_x)**2 + (pred_y - gt_y)**2\n",
    "                                err_str = f\"{err:7.4f}\"\n",
    "                            else:\n",
    "                                err_str = \"  n/a  \"\n",
    "\n",
    "                            print(f\"{t:4d} | {pred_str} | {gt_str} | {err_str}\")\n",
    "\n",
    "                        print(\"-\" * 70)\n",
    "                    # (No mean trajectory error / batch mean errors here, per your request)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752987c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TrajectoryDataset('datasets/goal_input_datasets/trajectory_dataset_test.json')\n",
    "\n",
    "N = len(dataset)\n",
    "train_size = int(0.8 * N)\n",
    "val_size = int(0.2 * N)\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, [train_size, val_size],\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,  \n",
    "    batch_size=8,\n",
    "    collate_fn=collate_pad,\n",
    "    shuffle=True,\n",
    "    num_workers=0  \n",
    ")\n",
    "        \n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=8,\n",
    "    collate_fn=collate_pad,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "model = lstm_trajectory_generator(input_size=3, trans_size=2, quat_size=2, hidden_size=128, num_layers=2)\n",
    "\n",
    "EPOCHS = 500\n",
    "history = train(model=model, train_loader=train_dataloader, val_loader=val_dataloader, epochs=EPOCHS, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e64dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists of 100 floats\n",
    "train_losses = history['train_loss']\n",
    "#val_losses = history['val_loss']\n",
    "\n",
    "epochs = range(1, EPOCHS+1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_losses, label='Train')\n",
    "#plt.plot(epochs, val_losses, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "il",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
